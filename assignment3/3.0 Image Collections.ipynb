{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3adcb898cb0d2e59e69e0f45149ac5d2",
     "grade": false,
     "grade_id": "cell-7076d2e60a7ca6b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Assignment 3\n",
    "This assignment consists of two notebook files. Each exercise is graded using *hidden* tests. If you pass these tests then you are rewarded *full points* for that exercise; if your code fails the tests in any way then you will get *no points* for that exercise. Make sure to **read the rules** before you start the assignment.\n",
    "\n",
    "## Rules\n",
    "For this assignment the following rules apply:\n",
    "\n",
    "**General**\n",
    " * The assignment should be completed in **groups of two or three** (enroll in a group on Brightspace).\n",
    " * Any kind of intergroup discussion will be considered fraud and both the parties will be punished.\n",
    " * All code must be written intra group. All external help, with the exception of Python/library documentation and the lecture slides, will be considered fraud (including generative AI).\n",
    " * Do not use libraries that implement the assignment for you. Ask a TA if you are unsure.\n",
    "\n",
    "**Grading**\n",
    " * Each exercise is graded using additional hidden tests. These tests will check your solution for different (unseen) inputs.\n",
    " * If the tests pass without error (warnings are allowed) then you receive full points.\n",
    " * If a test cell takes more than five minutes to complete then this is considered an error.\n",
    " * Do not make any assumptions on the input data (such as resolution) unless specified otherwise. Doing so may result in the tests failing and thus 0 points.\n",
    " * Your grade is computed as $\\frac{\\text{points}}{\\text{max\\_points}} * 9 + 1$ and will be rounded to the closest 0.1 point.\n",
    " * Submit your code to Brightspace as a zip file containing only the notebook (`*.ipynb`) files.\n",
    " * **Do not rename the notebook files**\n",
    " \n",
    "**Late Submissions**\n",
    " * Late submissions must be submitted *as soon as possible* to the \"Assignment 3 - Late Submissions\" assignment on Brightspace.\n",
    " * The following penalty will be applied: $\\text{adjusted grade} = \\text{grade} - 1 - \\lceil\\frac{\\text{minutes late}}{10}\\rceil$\n",
    "\n",
    "<br />\n",
    " \n",
    "**Before you submit**, make sure that you are not accidentaly using any global variables. Restart the kernel (wiping all global variables) and run the code from top to bottom by clicking \"Kernel\" => \"Restart & Run all\" in the menu bar at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "886147079dfe1253e19e045d365c02d5",
     "grade": false,
     "grade_id": "cell-91ecdd51e335450e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import scipy.signal\n",
    "import os\n",
    "# helpers.py is one level up in the directory structure so we need to tell Python were to find it\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import helpers\n",
    "local_data_folder = os.path.join(helpers.dataset_folder, \"week3\", \"collections\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "117e053cf0d4acd66df1ea1638c91db9",
     "grade": false,
     "grade_id": "cell-a54f8ec1286e81be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 0 (0 points)\n",
    "This is a group assignment. Every student is expected to contribute to all exercises.\n",
    "\n",
    "Enter the student IDs of all contributing students:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b8d5ffb3d11e8efbf26668a6954210d",
     "grade": false,
     "grade_id": "cell-49875c747627d70d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "student_ids = [\"1234\", \"5678\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5908f95d17a7236d2e8dc0612815c3e1",
     "grade": true,
     "grade_id": "cell-fc15b7a979cdfe33",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE, MODIFY, OR COPY THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f078f78d2c57e3a7efe0d61a0d542858",
     "grade": false,
     "grade_id": "cell-8315cc1f829f08c5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Searching image collections\n",
    "\n",
    "In the previous excercises we have seen how an image can be inpainted without access to additional information. Yet, such an approach is limited in the variety of content it can produce. We can achieve much richer and more constent inpainting by copying existing image content from a similar image. For that we need a source of such images - an image collection. Furthermore, we need a fast and reliable way how to find good matches to our query image.\n",
    "\n",
    "While we could potentially compare images pixel by pixel using the Sum of Squared Distances metric, a more efficient and often more robust approach is to utilize lower-dimensional image descriptors. One of the simplest choices is *GIST*.\n",
    "\n",
    "The name *GIST* refers to the idea of extracting the gist of the image. GIST is a simple fixed-length vector of real numbers that summarizes distribution of contrast (or gradients in the image). It is built by averaging intensity of edges detected in several image regions, at several scales and several orientations.\n",
    "\n",
    "In this assignment, we will compute GIST of an image and then use it to find similar images in a preprocessed image collection.\n",
    "\n",
    "### GIST algorithm\n",
    "There are several ways how to build a GIST descriptor. In this Assignment we apply a sequence of image convolutions (see Assignment 1.1) with differently oriented and sized Gabor filters. The results of each convolution are then split into a fixed-sized grid and averaged to obtain entries for the descriptor. Finally, a low resolution version of the image is appended to the final descriptor to encode the average color distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60f26ea045b9ef87d891ea86d4d08617",
     "grade": false,
     "grade_id": "cell-8cf1d83c8ef069f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Gabor Filter\n",
    "\n",
    "The coefficients of the GIST descriptor are composed of image gradients detected at different scales (representing range of spatial frequencies) and different orientations. \n",
    "\n",
    "We have previously used specialized filters to detect sharp edges (high frequency) with either horizontal or vertical orientation. \n",
    "Gabor filter (or Gabor patches) are a generalization of edge detection that is inspired by processing in human visual cortex.\n",
    "\n",
    "\n",
    "<img src=\"gabor.png\" alt=\"Gabor filter\" style=\"width:20%\">\n",
    "\n",
    "\n",
    "A Gabor filter $f(x,y)$ is a product of a symmetrical bi-variate Gaussian filter\n",
    "$$\n",
    "g(x,y) = e^{-\\frac{1}{2}\\frac{x^2+y^2}{\\sigma^2}}\n",
    "$$\n",
    "and a sinusoidal corrugation\n",
    "$$\n",
    "s(x,y) = \\sin\\left((x \\cos(\\alpha) - y \\sin(\\alpha)) \\cdot f\\right)\n",
    "$$\n",
    "with parameters\n",
    "- $\\sigma$ as a standard deviation controlling the width of the filter,\n",
    "- $\\alpha$ as an angle (in radians) controlling the orientation of the filter (and detected gradients)\n",
    "- $f$ as a spatial frequency controling the sensitivity of the filter to sharp or smooth gradients.\n",
    "\n",
    "Finally, the entire Gabor filter is\n",
    "$$\n",
    "f(x,y) = g(x,y) \\cdot s(x,y) \\cdot \\frac{1}{\\sum_{x,y} g(x,y)}\n",
    "$$\n",
    "where the denominator on the right normalizes the sum of the Gaussian component to one. \n",
    "\n",
    "This normalization ensures that the response of the filter stays normalized regardless of the discretization to fixed-resolution pixel grid of size $s$.\n",
    "\n",
    "### Tips\n",
    "Try how the filter changes for different choices of $\\sigma$, $\\alpha$ and $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_gabor_filter(size, sigma, f, alpha):\n",
    "    x, y = np.meshgrid(np.linspace(-1,1,size),np.linspace(-1,1,size))\n",
    "    ox = x * np.cos(alpha) - y * np.sin(alpha)\n",
    "    r2 = x**2 + y**2\n",
    "    gauss = np.exp(-0.5*r2/sigma**2)\n",
    "    gauss /= gauss.sum()\n",
    "    wave = np.sin(ox*f)\n",
    "    gabor_filter = gauss * wave\n",
    "    return gabor_filter\n",
    "    \n",
    "\n",
    "gabor_filter = build_gabor_filter(35, 1.0/3.2, 8, np.radians(60))\n",
    "helpers.show_image(gabor_filter / gabor_filter.max() * 0.5 + 0.5, \"Your Gabor Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c13f92a2430d5d07fbc32d55d0ea50bc",
     "grade": false,
     "grade_id": "cell-2602ee05bb637eed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 (4 points)\n",
    "\n",
    "The next step is to apply the filter $f(x,y)$ to a grayscale image $I(x,y)\\in\\mathcal{R}^{W\\times H}$ using convolution. The Gabor filter response $r(x,y) = I(x,y) * f(x,y)$ has a sign that depends on the mutual orientation of the filter and the gradient. In this excercise the sign does not matter and therefore we return absolute value of the response $r_a(x,y) = |r(x,y)|$.\n",
    "\n",
    "The filter response is expected to have the same resolution as the input image. However, we need the GIST descriptor to be relatively small. Therefore, we bin the response values into a `grid_size` $\\times$ `grid_size` 2D histogram $H(i,j)$ where each bin contains the mean of $r_a(x,y)$ for the pixels that are covered by that bin.\n",
    "\n",
    "### Task\n",
    "First, implement a method that for a given grayscale image returns an absolute Gabor filter response.\n",
    "\n",
    "Second, implement a method that bins this response into a 2D histogram as defined above. This function should work for both grayscale images and RGB color images (in which case the binned image should also have 3 color channels).\n",
    "\n",
    "### Tips\n",
    "- You can assume that the image size is divisible by the `grid_size`.\n",
    "- You are allowed to use `scipy.signal.convolve2d(image, kernel, mode=\"same\")`\n",
    "- Exlore how the response of the filter changes for different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af3ed732b2b282ef5e8c9f824f0f2afb",
     "grade": false,
     "grade_id": "exercise1_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_absolute_response(image, filter_size, sigma, f, alpha):\n",
    "    gabor_filter = build_gabor_filter(filter_size, sigma, f, alpha)\n",
    "    # TODO: Compute the absolute value of the gabor filter applied to the image.\n",
    "    abs_response = image.copy()\n",
    "    # YOUR CODE HERE\n",
    "    return abs_response, gabor_filter\n",
    "    \n",
    "def bin_values(image, grid_size):\n",
    "    if len(image.shape) == 2:\n",
    "        # Grayscale image\n",
    "        binned_values = np.zeros((grid_size, grid_size))\n",
    "    else:\n",
    "        # Color image\n",
    "        binned_values = np.zeros((grid_size, grid_size, image.shape[2]))\n",
    "    \n",
    "    # TODO: Bin the input image as described in the exercise description.\n",
    "    # YOUR CODE HERE\n",
    "    return binned_values\n",
    "\n",
    "grayscale_image = helpers.imread_normalized_float_grayscale(os.path.join(local_data_folder, \"grad_test.png\"))\n",
    "response, gabor_filter = get_absolute_response(grayscale_image, 32, 1.0/3.2, 8, np.radians(45))\n",
    "binned = bin_values(response, 4)\n",
    "\n",
    "helpers.show_images({\n",
    "    'Input Image': grayscale_image,\n",
    "    'Your Filter': gabor_filter / gabor_filter.max() * 0.5 + 0.5,\n",
    "    'Your Response': response / response.max(),\n",
    "    'Your Binned': plt.get_cmap(\"gray\")(binned / max(binned.max(), 0.0000001))\n",
    "}, nrows=1, ncols=4)\n",
    "\n",
    "color_image = helpers.imread_normalized_float(os.path.join(local_data_folder, \"opencountry_land493.jpg\"))\n",
    "response = bin_values(color_image, 4)\n",
    "helpers.show_images({\n",
    "    'Input Color Image': color_image,\n",
    "    'Your Binned': response\n",
    "}, nrows=1, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b0e127eda02c0a575663c8f28033f8f",
     "grade": false,
     "grade_id": "cell-46a068273c0b7547",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Testing your solution of exercise 1\n",
    "You can test your `bin_values(vals, grid_size)` function by creating a small and simple test image. Work out by hand what the binned image should look like and verify that your solution matches what you expect. The code below checks some basic properties such as the size of the returned image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own tests here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9950be9239eab2ca8c20923a96153507",
     "grade": true,
     "grade_id": "exercise1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE, MODIFY, OR COPY THIS CELL\n",
    "\n",
    "image = helpers.imread_normalized_float_grayscale(os.path.join(local_data_folder, \"grad_test.png\"))\n",
    "response, gabor_filter = get_absolute_response(image, 32, 1.0/3.2, 8, np.radians(45))\n",
    "assert(gabor_filter.shape == (32, 32))\n",
    "assert(response.shape == image.shape)\n",
    "binned = bin_values(response, 8)\n",
    "assert(binned.shape == (8, 8))\n",
    "\n",
    "image = helpers.imread_normalized_float(os.path.join(local_data_folder, \"opencountry_land493.jpg\"))\n",
    "binned = bin_values(image, 16)\n",
    "assert(binned.shape == (16, 16, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edd4cc1f8a60af75edadc36d59cfa483",
     "grade": false,
     "grade_id": "cell-d6e05bc8fffb4433",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2 (5 points)\n",
    "Finally, a *GIST* descriptor is formed by concatenation of flattened histograms $H_k$ for Gabor filters $g_k$ with different scales and orientations. Here is an example of a small filter bank with 3 different sizes $s$,= and 4 orientations $\\alpha$:\n",
    "\n",
    "<img src=\"gabors_4x3.png\" alt=\"Gabor filters\" style=\"width:60%\" />\n",
    "\n",
    "\n",
    "If we further consider a histogram size of $2\\times2$, we obtain a GIST descriptor as a 1D vector with $3\\times4\\times2\\times2 = 48$ values (for this particular choice of parameters). \n",
    "The GIST vector features one entire histogram row after another and this is repeated first for every orientation $\\alpha$ and then for every scale $s$, so that the order is:\n",
    "$$\n",
    "[s_0,\\alpha_0,y_0,x_0], [s_0,\\alpha_0,y_0,x_1],\\cdots[s_0,\\alpha_0,y_1,x_0],\\cdots[s_0,\\alpha_1,y_0,x_0],\\cdots[s_1,\\alpha_0,y_0,x_0],\\cdots[s_S,\\alpha_A,y_Y,x_X].\n",
    "$$\n",
    "\n",
    "The GIST alone only describes the distribution of gradients in the image. To make it more powerfull, we append one more histogram describing the distribution of the colors in the image similarly as shown for the collection-based inpainting in the lecture.\n",
    "This color histogram is built the same way as previously but instead of the filter response it averages the RGB value of the image. When appeneded at the end of the descriptor, it adds another `grid_size` $\\times$ `grid_size` $\\times 3$ values that correspond to a flattened 2D color histogram with an order:\n",
    "\n",
    "$$\n",
    "[y_0,x_0,R],[y_0,x_0,G],[y_0,x_0,B],\\cdots[y_0,x_1,R],[y_0,x_1,G],[y_0,x_1,B],\\cdots[y_1,x_0,R],[y_1,x_0,G],[y_0,x_1,B],\\cdots\n",
    "$$\n",
    "\n",
    "The final descriptor for this assignment is then defined as\n",
    "$\\mathbf{d} = [\\textrm{GIST} \\cdot w_{grad}, H_{RGB}]$\n",
    "where $w_{grad}$ is additional weight scalar that scales the contribution of the gradients.\n",
    "\n",
    "### Task\n",
    "Implement a method that computes a full descriptor of given RGB image including the RGB appendix. \n",
    "Cover all combinations of sizes $s$ and orientations $\\alpha$.\n",
    "\n",
    "### Tips\n",
    "- The histogram method developed in previous excercise can be easily adapted to compute the RGB histogram.\n",
    "- Visualize the individual filters and their responses in a similar grid as shown in the exercise to better understand what each filter scale and orientation does.\n",
    "\n",
    "**NOTE**: For grading we use the reference implementation of `get_absolute_response()` and `bin_values()` to ensure that you are not punished for mistakes in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "165fd531cd3cc23d3ac07bee4e17562b",
     "grade": false,
     "grade_id": "cell-36f1a57a16bd2ebd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def image_rgb_to_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def build_gist_descriptor(image, filter_sizes, alphas, sigma, f, grid_size, grad_weight):\n",
    "    # Use image_rgb_to_grayscale() to convert the RGB image to grayscale.\n",
    "    descriptor = np.zeros((len(filter_sizes) * len(alphas) + 3) * grid_size*grid_size)\n",
    "    image_grayscale = image_rgb_to_grayscale(image)\n",
    "    # TODO: fill the GIST descriptor as descriptor as described in the exercise description.\n",
    "    # YOUR CODE HERE\n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "584fac3ee1a7deb26c4798ac8f030d82",
     "grade": false,
     "grade_id": "cell-3a827c565358ac08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Testing your solution of exercise 2\n",
    "Below is a visualization of the different parts of the image descriptor that you've generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own tests here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = helpers.imread_normalized_float(os.path.join(local_data_folder, \"spatenv/opencountry_land228.png\"))\n",
    "\n",
    "filter_sizes = np.array([9,17,33,65])\n",
    "alphas = np.radians(np.arange(6, dtype=float)/6*180)\n",
    "sigma = 1.0 / 3.2\n",
    "f = 8\n",
    "grid_size = 4\n",
    "grad_weight = 20\n",
    "descriptor = build_gist_descriptor(image, filter_sizes, alphas, sigma, f, grid_size, grad_weight)\n",
    "\n",
    "# Visualize.\n",
    "dsize = grid_size**2\n",
    "s_maxs = np.array([descriptor[i*(dsize*len(alphas)):(i+1)*(dsize*len(alphas))].max() for i in range(len(filter_sizes))])\n",
    "panels = {}\n",
    "for i, (s, a) in enumerate(itertools.product(filter_sizes, alphas)):\n",
    "    d = descriptor[i*dsize:(i+1)*dsize].reshape(grid_size, grid_size)\n",
    "    panels[f's={s} a={np.degrees(a):.0f}'] = plt.get_cmap('viridis')(d / s_maxs[filter_sizes==s])\n",
    "panels['RGB'] = descriptor[-(dsize*3):].reshape(grid_size,grid_size,3)\n",
    "skeys = ['s=9 a=0','s=17 a=60', 's=33 a=90', 's=65 a=120', 'RGB']\n",
    "panels = {f'Your {k}': panels[k] for k in skeys}\n",
    "panels.update({ \"Input\": image })\n",
    "helpers.show_images(panels, ncols=len(panels), col_height=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bf720813b30de2ad18aea57a23ac1e8",
     "grade": true,
     "grade_id": "exercise3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE, MODIFY, OR COPY THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8153aaf2f15c7c66e3718afe1631323",
     "grade": false,
     "grade_id": "dgdfgdfgdfg342",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3 (4 points)\n",
    "\n",
    "We can now use the resulting descriptor to quickly find a set of $k$ most similar images in potentially large image collection which would take a long time to compare pixel-by-pixel. \n",
    "\n",
    "In this exercise we will find the most similar image by minimizing the sum of squared distances between the query image descriptor $\\mathbf{d}$ and the collection image descriptors $\\mathbf{d}_i$ such that the index of the most similar image $j$ is\n",
    "$$\n",
    "j = \\arg\\min_{i} ||\\mathbf{d} - \\mathbf{d}_i||^2_2.\n",
    "$$\n",
    "\n",
    "This is useful for exploring large image collections where a search engine can suggest other images similar to the one we are looking at. It can also be used for a coarse categorization of images (coast, mountains, road, city,...) and in the lecture we have seen that a similar image can be used for image inpainting. \n",
    "\n",
    "Note, that for an effective implementation, we must ignore the pixels that are marked as invalid in a provided inpainting mask $M$. That means that every convolution has to carefully avoid sampling the invalid pixels which results in a more complex and slow implementation This is why in this exercise we simulate similar effect by modifying the query image.\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement a method that replaces marked pixels in a given RGB image $\\mathbf{I} \\in \\mathcal{R}^{H \\times W \\times 3}$ with pixels from the most similar image in a provided image collection. \n",
    "The marked pixels are encoded by zeros in the provided mask $M \\in [0,1]^{H \\times W}$.\n",
    "Find the most similar image by minimizing the L2 descriptor norm as shown above.\n",
    "To compute the query image descriptor, use a modified query image generated from the input image by replacing the invalid pixels ($M = 0$) by a uniform gray color ($[0.5, 0.5, 0.5]$).\n",
    "Return the modified query image, the most similar collection image and the inpainted image.\n",
    "\n",
    "\n",
    "### Tips\n",
    "- The descriptors for all images in the dataset are provided and do not need to be recomputed.\n",
    "- Visualize several of the most similar and also several of the least similar images. \n",
    "- Test the query with your own image. Note that the metric is only effective if the collection contains a very close match of the query image. In practice, that means that collections of milions of images are needed. In this example we only use a very small collection of pre-selected image samples.\n",
    "\n",
    "Images courtesy of [8 Scene Categories Dataset](http://people.csail.mit.edu/torralba/code/spatialenvelope/)\n",
    "\n",
    "**NOTE**: For grading we use the reference implementation of `build_gist_descriptor()` to ensure that you are not punished for mistakes in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f747b1317e340f425d11da4e36db21d",
     "grade": false,
     "grade_id": "exercise4_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inpaint_image(image, mask, dataset_descriptors, dataset_files, \n",
    "                 filter_sizes, alphas, sigma, f, grid_sizes, grad_weight):\n",
    "    # Use helpers.imread_normalized_float(dataset_files[i]) to read an image i\n",
    "    masked_query_image = image.copy()\n",
    "    nearest_image = helpers.imread_normalized_float(dataset_files[0]) # First image in the dataset\n",
    "    inpainted_image = image.copy()\n",
    "\n",
    "    # TODO 1: Set the masked pixels in masked_query_image to gray ([0.5, 0.5, 0.5])\n",
    "    # TODO 2: Compute the GIST descriptor for masked_query_image and select the closest descriptor from dataset_descriptors.\n",
    "    # TODO 3: Load the corresponding image from dataset_files (use helpers.imread_normalized_float) into nearest_image\n",
    "    # TODO 4: Use nearest_image to inpaint the masked part of the image (store the result in inpainted_image).\n",
    "    # YOUR CODE HERE\n",
    "    return masked_query_image, nearest_image, inpainted_image\n",
    "\n",
    "filter_sizes = np.array([9,17,33,65])\n",
    "alphas = np.radians(np.arange(6, dtype=float)/6*180)\n",
    "sigma = 1.0 / 3.2\n",
    "f = 8\n",
    "grid_size = 4\n",
    "grad_weight = 20\n",
    "\n",
    "dataset_files = sorted(helpers.list_files_in_folder(os.path.join(local_data_folder, \"spatenv\"), filter=\"*.png\"))\n",
    "dataset_descriptors = np.load(os.path.join(local_data_folder, 'spatenv_descs.npy'))\n",
    "image = helpers.imread_normalized_float(os.path.join(local_data_folder, \"opencountry_land493.jpg\"))\n",
    "mask = np.zeros(image.shape[:2])\n",
    "mask[:image.shape[0]//2] = 1\n",
    "\n",
    "query, nearest, inpainted = inpaint_image(image.copy(), mask, dataset_descriptors, dataset_files,\n",
    "                                        filter_sizes, alphas, sigma, f, grid_size, grad_weight)\n",
    "                                        \n",
    "helpers.show_images({\n",
    "    'Your Input': image,\n",
    "    'Your Query': query,\n",
    "    'Your Nearest': nearest,\n",
    "    'Your Result': inpainted\n",
    "}, nrows=1, ncols=4, col_height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6183efe36969f18a57165b13f3f0440c",
     "grade": false,
     "grade_id": "cell-8075b99c15122bf3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Testing your solution of exercise 3\n",
    "Have a look at the set of images in the data set (`datasets/week3/collections/spatenv/` folder). Is the image picked by your algorithm close to the query image (compared to the other images in the data set)? Try different query images and check if this is still the case.\n",
    "\n",
    "The code below checks some basic properties such as the size of the returned images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own tests here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0fd70224a38e39dced38b126ebcb259",
     "grade": true,
     "grade_id": "exercise4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE, MODIFY, OR COPY THIS CELL\n",
    "\n",
    "filter_sizes = np.array([9,17,33,65])\n",
    "alphas = np.radians(np.arange(6, dtype=float)/6*180)\n",
    "sigma = 1.0 / 3.2\n",
    "f = 8\n",
    "grid_size = 4\n",
    "grad_weight = 20\n",
    "\n",
    "dataset_files = sorted(helpers.list_files_in_folder(os.path.join(local_data_folder, \"spatenv\"), filter=\"*.png\"))\n",
    "dataset_descriptors = np.load(os.path.join(local_data_folder, 'spatenv_descs.npy'))\n",
    "image = helpers.imread_normalized_float(os.path.join(local_data_folder, \"opencountry_land493.jpg\"))\n",
    "mask = np.zeros(image.shape[:2])\n",
    "mask[:image.shape[0]//2] = 1\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    query, nearest, inpainted = inpaint_image(image.copy(), mask, dataset_descriptors, dataset_files,\n",
    "                                        filter_sizes, alphas, sigma, f, grid_size, grad_weight)\n",
    "assert(query.shape == image.shape)\n",
    "assert(nearest.shape == image.shape)\n",
    "assert(inpainted.shape == image.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
