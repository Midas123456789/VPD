{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b581647df4ff3f7962aa2e43f915e024",
     "grade": false,
     "grade_id": "cell-9db57c82d6ddbb3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.0 or less. Got NumPy 2.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrMethodFormatter\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anand\\Documents\\cs_minor\\VPD\\VPD\\.venv\\Lib\\site-packages\\numba\\__init__.py:59\u001b[0m\n\u001b[0;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[1;32mc:\\Users\\anand\\Documents\\cs_minor\\VPD\\VPD\\.venv\\Lib\\site-packages\\numba\\__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.0 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 2.0 or less. Got NumPy 2.2."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import math\n",
    "import numpy as np\n",
    "print(np.version)\n",
    "import numba\n",
    "import cv2\n",
    "import scipy.signal\n",
    "import scipy.ndimage\n",
    "import progressbar # pip install progressbar2\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import os\n",
    "import sys\n",
    "from numba import jit, njit\n",
    "sys.path.append(\"../../\")\n",
    "import helpers\n",
    "\n",
    "# Init plotly\n",
    "py.init_notebook_mode()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bb4601f114a22092dc8514ee4e9da24",
     "grade": false,
     "grade_id": "cell-d99a945cc738f05d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 3D Reconstruction\n",
    "In this notebook we look at generating a virtual representation from a set of (real world) images. Given a collection of images from different angles the aim is to create a 3D reconstruction of the scene that the images capture. Common use cases include augmented reality, [virtual cameras](https://www.youtube.com/watch?v=J7xIBoPr83A) and [photogrammetry](https://www.youtube.com/watch?v=wnt64H-Wouk).\n",
    "\n",
    "## Pipeline\n",
    "Estimating the camera matrix is the first step of 3D reconstruction. As you learned in the previous notebook we can compute the camera matrix given a list of known pixels/3D point pairs. In real-world photogrammetry software this process is completely automated. However this gives us only a very sparse representation: typically only a couple hundred points for images with millions of pixels. In this notebook we will compute a 3D point for each pixel in the image, assuming that the camera matrices are already known.\n",
    "\n",
    "### Setup\n",
    "For all the following exercises we will use the data set that is made publicly available by [ETH Zurich](https://www.eth3d.net/datasets). This data set already includes the camera matrices as well as a set the set of 3D points and their location in the images. The following code loads 2 images looking at the same object from different angles, including the camera matrices and the set of 3D points used to compute these matrices (displayed as colored dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b4c2bba56ca6af21c0f88db85399757",
     "grade": false,
     "grade_id": "cell-8974d74bcb637759",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mvs\n",
    "import eth3d_loader\n",
    "import glm\n",
    "\n",
    "try:\n",
    "    glm.vec2(2)\n",
    "except:\n",
    "    print(\"================\")\n",
    "    print(\"===== Error ====\")\n",
    "    print(\"================\")\n",
    "    print(\"It seems like pyglm could not be accessed.\") \n",
    "    print(\"This is most commonly caused by having both the \\\"pyglm\\\" and \\\"glm\\\" libraries installed at the same time.\")\n",
    "    print(\"These libraries are incompatible with eachother because both use \\\"import glm\\\"\")\n",
    "    print(\"Please check in Anaconda Navigator that pyglm IS installed and glm IS NOT installed, then restart the notebook (Kernel -> Restart Kernel)\")\n",
    "\n",
    "# https://www.eth3d.net/datasets\n",
    "# https://www.eth3d.net/documentation#camera-models-pinhole-camera-model\n",
    "points3D, (camera0, camera1), (image0, image1), (points2D_0, points2D_1) = eth3d_loader.load_eth3d_dataset(\n",
    "    os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"eth3d\", \"statue\"), [\"4\", \"5\"], scale=0.1, swap_uv=True)\n",
    "_, (camera0_low_res, camera1_low_res), (image0_low_res, image1_low_res), _ = eth3d_loader.load_eth3d_dataset(\n",
    "    os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"eth3d\", \"statue\"), [\"4\", \"5\"], scale=0.05, swap_uv=True)\n",
    "\n",
    "# Convert from list of glm.vec3 to Numpy format.\n",
    "points2D_0_np = np.array(points2D_0)\n",
    "points2D_1_np = np.array(points2D_1)\n",
    "# Generate random colors.\n",
    "colors = np.random.rand((len(points2D_1_np)), 3)\n",
    "\n",
    "num_points_to_show = 20\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(image0)\n",
    "ax1.scatter(points2D_0_np[:num_points_to_show, 0], points2D_0_np[:num_points_to_show, 1], c=colors[:num_points_to_show])\n",
    "ax2.imshow(image1)\n",
    "ax2.scatter(points2D_1_np[:num_points_to_show, 0], points2D_1_np[:num_points_to_show, 1], c=colors[:num_points_to_show])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9861527e9d20e53ff7e31a4bcaedb325",
     "grade": false,
     "grade_id": "cell-583d2aa82725bf58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Camera location and references points\n",
    "The following 3D plot illustrates the location of the two cameras relative to each other as well as to the statue. The known 3D points (part of the dataset) are plotted as orange dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9c808f59b34d1dd03812c24c1153d47",
     "grade": false,
     "grade_id": "cell-cb98d291bd0d1073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mvs.plot_cameras_3D(image0, camera0, camera1, points3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "467a0fe55dcbe52f4e5fd83a052c8a5a",
     "grade": false,
     "grade_id": "cell-9e53951f2152e2a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Triangulating matching view rays\n",
    "Each pixel defines a ray in 3D space. A ray is a half open light segment: it starts at the camera and it goes through the pixel towards infinity. A ray is defined by the following formula: $\\vec{p}=\\vec{o}+t\\vec{d}$. Here, $\\vec{o}$ is the origin (starting point) and $\\vec{d}$ is the direction of the ray. The direction vector is normalized $\\lvert \\vec{d} = 1\\rvert$ such that $t$ describes the length of the ray.\n",
    "\n",
    "If we know that two pixels are looking at the same point in 3D then it is possible to compute the position of that 3D point based on the pixel coordinates in both images. Imagine the two rays corresponding to the matching pixels: one starting from `camera0` and the other at `camera1`. The rays will intersect at that point in 3D space. However due to various sources of error (discretization and computer arithmatic) it is very uncommon for two rays to exactly intersect. So instead we find the closest point half-way between the two rays.\n",
    "\n",
    "Given two rays $\\vec{p}=\\vec{o_p}+t_p\\vec{d_p}$ and $\\vec{q}=\\vec{o_q}+t_q\\vec{d_q}$ we want to find the closest point between them. Two rays are closest when the line segment between $p$ and $q$ ($\\vec{pq} = \\vec{q} - \\vec{p}$) is orthogonal to both rays: $\\vec{d_p} \\cdot pq = 0$ and $\\vec{d_q} \\cdot pq = 0$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{d_p} \\cdot pq &= 0 \\text{ and } \\vec{d_q} \\cdot pq = 0 \\\\\n",
    "\\vec{d_p} \\cdot (\\vec{o_q} + t_q\\vec{d_q} - \\vec{o_p} - t_p\\vec{d_p}) &= 0 \\text{ and } \\vec{d_q} \\cdot (\\vec{o_q} + t_q\\vec{d_q} - \\vec{o_p} - t_p\\vec{d_p}) = 0 \\\\\n",
    "\\vec{d_p}\\cdot\\vec{d_q}t_q - \\vec{d_p}\\cdot\\vec{d_p}t_p &= \\vec{o_p}\\cdot\\vec{d_p} - \\vec{o_q}\\cdot\\vec{d_p} \\text{ and } \\vec{d_q}\\cdot\\vec{d_q}t_q - \\vec{d_p}\\cdot\\vec{d_q}t_p = \\vec{o_p}\\cdot\\vec{d_q} - \\vec{o_q}\\cdot\\vec{d_q} \\\\\n",
    "... &= ...\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### GLM library\n",
    "In the following exercises we will be using the `glm` library to store our three dimensional vectors (`glm.vec3`) rather than `numpy` due to performance reasons. These work the exact same way as `numpy` arrays (element wise operations) and are interoperable with `numpy` (you can call `numpy` functions such as `np.dot` on a `glm.vec3`).\n",
    "\n",
    "### Exercise 4 (2 points)\n",
    "Derive the exact solution to $t_p$ and $t_q$ by hand using the formula above as a starting point. Then implement a function that uses this derivation to compute the values for $p$ and $q$.\n",
    "\n",
    "Calling `np.linalg.lstsq` (or similar function) is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6583cd5a6880d24760459de5bab5d9dd",
     "grade": false,
     "grade_id": "exercise8_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the closest points p and q on ray 0 and ray 1\n",
    "def closest_points_on_rays(ray0, ray1):\n",
    "    o0, d0 = ray0\n",
    "    o1, d1 = ray1\n",
    "    \n",
    "    # TODO: Compute the point p on ray0 which is closest to ray1; Similarly, compute the point q on ray1 which is closest to ray0.\n",
    "    \n",
    "    a0 = (o0 * d1 - o1 * d1) / (d0 * d1)\n",
    "    a1 = (- o0 * d0 - o0 * d0 +o1 * d0 + o0 * d1 ) / (1 - d0 * d1)\n",
    "    \n",
    "    p = o0 + (a2 - a1)/2 * d0\n",
    "    q = o1 + (a1 + a1)/2 * d1\n",
    "    # YOUR CODE HERE\n",
    "    return (p, q)\n",
    "\n",
    "# Find the closest point between both rays (half way between p and q)\n",
    "def closest_point_between_rays(ray0, ray1):\n",
    "    p, q = closest_points_on_rays(ray0, ray1)\n",
    "    return (p + q) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bf81440b71f557e495918250b857973",
     "grade": false,
     "grade_id": "cell-f8b5c6ff55b853c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test of exercise 4\n",
    "We provide some code to help you verify your solution. As part of the provided dataset we obtain a set of 3D points that are known to lie on the surface of the object. Each such point maps to a different pixel in both images. We shoot a ray for each camera \"through\" their respective pixel and call your function to find the 3D location where they intersect. Your computed 3D point should be very close to the actual known 3D point position.\n",
    "\n",
    "The graph plots the percentage of points which have an error (in centimeters) smaller than the value indicated on the horizontal axis. *Your solution should be accurate to 10cm.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own tests here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triangulation_errors = []\n",
    "for pixel0, pixel1, ref_point3D in zip(points2D_0, points2D_1, points3D):\n",
    "    ray0 = mvs.pixel_to_ray(camera0, pixel0)\n",
    "    ray1 = mvs.pixel_to_ray(camera1, pixel1)\n",
    "    point3D = closest_point_between_rays(ray0, ray1)\n",
    "    triangulation_errors.append(glm.length(point3D - ref_point3D))\n",
    "\n",
    "x = np.linspace(0, 100, 100)\n",
    "y = np.percentile(triangulation_errors, x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=helpers.default_fig_size)\n",
    "ax.set_title(\"Percentile of points with an error (estimated vs actual position) smaller than \\\"Error (cm)\\\"\")\n",
    "ax.set_xlabel(\"Error (cm)\")\n",
    "ax.set_ylabel(\"Percentile of points\")\n",
    "ax.plot(y*100, x)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a4fdac6b796829b48408c9b52d0ac63",
     "grade": true,
     "grade_id": "exercise8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE, MODIFY, OR COPY THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef2455a037370b0ac280d14c4bfe5577",
     "grade": false,
     "grade_id": "cell-62f7a9c4f3d2358c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Epipolar lines\n",
    "To create a 3D reconstruction from two images we have to find matching pixel pairs. Comparing each pixel in one image with each pixel in the other image is not only inefficient but might also lead to matches who's rays do not get close to intersecting.\n",
    "\n",
    "By projecting the ray from one camera into the image of the other camera we can guarantee that we only consider pixels with intersecting rays. The projection of the ray becomes a line in the other image which we refer to as an *epipolar line*.\n",
    "\n",
    "The demo below allows you to interactively visualize the epipolar line. **Click anywhere in the left image** to select a pixel. The matching epipolar line will be drawn in the right image. Verify that the epipolar line indeed crosses the 3D point that you clicked in the left image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "267e12ff78ea1376dfa067abc8e2b7a3",
     "grade": false,
     "grade_id": "cell-c0d75fc433c17a85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mvs.plot_interactive_epipolar_lines(image0, image1, camera0, camera1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b83ae9d114b811a06ce038e675e578e0",
     "grade": false,
     "grade_id": "cell-bff127e77df0d9d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Photo consistency measures\n",
    "To detect which pixels are seeing the same 3D point, we need a way of expressing the similarity between two pixels (in two different images). A single pixel (3 values in RGB) does not capture enough information to find the correct match out of thousands of potential pixels. Instead we look at a little square patch around the pixel of interest (similar to examplar based inpainting).\n",
    "\n",
    "A photo consistency measure is a function that compares two patches and returns a score indicating how similar they are.  The Sum of Absolute Differences (SAD) function considers the average error: $\\sum\\limits_i^N \\lvert x_i - y_i \\rvert / N$, where $\\sum\\limits_i^N$ iterates over both pixels **and colour channels** in the patch, and $N$ is the number of pixels times the number of colour channels. This means that a large deviation in a single pixel will give the same results as a slight devation in all pixels. The Sum of Squared Differences (SSD) fixes this issue by punishing large deviations: $\\sum\\limits_i^N (x_i - y_i)^2/N$. Both SAD and SSD return an error rather than a similarity score: a *high* SAD/SSD score means *low* similarity. A photo consistency function should return a similarity score thus the $SAD$ and $SSD$ result must be negated: $-SAD$ and $-SSD$.\n",
    "\n",
    "A slightly more advanced photo consistency measure is Normalised Cross Correlation (NCC) which is defined as followed:\n",
    "\n",
    "$$\n",
    "norm\\_corr(x,y)=\\dfrac{\\sum\\limits_i^N (x_i - \\overline{x}) (y_i - \\overline{y}) }{\\sqrt{\\sum\\limits_i^N (x_i - \\overline{x})^2 \\sum\\limits_i^N (y - \\overline{y})^2}}\n",
    "$$\n",
    "\n",
    "where $\\overline{x}$ and $\\overline{y}$ are the mean values over $x$ and $y$ respectively.\n",
    "\n",
    "### Exercise 5 (1 point)\n",
    "Implement the SAD, SSD and NCC photo consistency measures to compare two equally sized RGB patches. *Make sure that all functions return higher values for higher similarities*.\n",
    "\n",
    "**NOTE** The photo consistency functions have an `@njit` decorator. This means that these functions will be compiled to native machine instructions by the Numba compiler rather than the default Python interpreter. This dramatically increases performance over regular Python, which is required for 3D reconstruction. However Numba is more picky than the regular Python interpreter and its error messages are more convoluted. You may remove the `@njit` decorator if needed but be aware that doing so will break the final 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52c96bbf34afa8fa16d04d29b7e07260",
     "grade": false,
     "grade_id": "exercise9_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def photo_consistency_SAD(patch0, patch1):\n",
    "    # TODO: Implement photo-consistency based on the sum of *absolute* differences.\n",
    "    # YOUR CODE HERE\n",
    "    return 0.0\n",
    "\n",
    "@njit\n",
    "def photo_consistency_SSD(patch0, patch1):\n",
    "    # TODO: Implement photo-consistency based on the sum of *squared* differences.\n",
    "    # YOUR CODE HERE\n",
    "    return 0.0\n",
    "\n",
    "@njit\n",
    "def photo_consistency_NCC(patch0, patch1):\n",
    "    # TODO: Implement photo-consistency based on the normalized cross correlation.\n",
    "    # YOUR CODE HERE\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f192817018a32c5b71adff0b65681105",
     "grade": false,
     "grade_id": "cell-8f2df16325d4946d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test of exercise 5\n",
    "The plot below visualizes the response of a photo consistency function along the epipolar line for one of the known 3D points. The pixel on the epipolar line with the highest photo consistency score is indicated in *green*. Since the 3D point is known we also have a reference which is highlighted in *red*.\n",
    "\n",
    "Try out different photo consistency functions and support domain sizes for different 3D points. The photo consistency will not always be correct but it should be a good estimate in most cases, see the paragraph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own tests here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80cfe8023fd666b6e5ebc0e044b84d8",
     "grade": false,
     "grade_id": "cell-f492e87a5baa4c0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pc_functions = {\n",
    "    \"SSD\": photo_consistency_SSD,\n",
    "    \"SAD\": photo_consistency_SAD,\n",
    "    \"NCC\": photo_consistency_NCC,\n",
    "}\n",
    "\n",
    "mvs.plot_photoconsistency_along_ray(image0, image1, camera0, camera1, points2D_0, points3D, pc_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce4238d2bdff23e0b3b473a21702431",
     "grade": true,
     "grade_id": "exercise9_correct",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE, MODIFY, OR COPY THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3138a9b52fd657155b68ec1576db26f8",
     "grade": false,
     "grade_id": "cell-d3d0e07b12133c67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Photo consistency accuracy\n",
    "To get a better idea of how the photo consistency measures perform overall we compare the results for all known 3D points. The following percentile graph shows the percentage of points whose photo consistency peak lies within a certain distance of the correct pixel (distance measured in pixels in an image). Note that a photo consistency measure is not perfect: not all pixels will match to the correct points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4051896e5888088a68c16509dd95fd7c",
     "grade": false,
     "grade_id": "cell-3dc0916b9f2b54a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mvs.plot_photoconsistency_accuracy(image0, image1, camera0, camera1, points2D_0[:100], points2D_1[:100], points3D, pc_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd2cb47c32bbbe5a70934eacaa1655bb",
     "grade": false,
     "grade_id": "cell-59b5e6f279fb5702",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Interactive Photoconsistency Demo\n",
    "The following live demo shows everything in action. **Click anywhere in the left image** and the program will automatically compute and draw the photo consistency along the epipolar line in the right image. It will also highlight the pixel with the highest photo consistency measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ee2b546ebcff42b245c786ffe02c47a",
     "grade": false,
     "grade_id": "cell-24b88c06c28468d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mvs.plot_epipolar_line_with_photoconsistency(image0, image1, camera0, camera1, 0.1, 10.0, pc_functions, closest_point_between_rays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fd4ea2c0159aa385185210c435c972c",
     "grade": false,
     "grade_id": "cell-50d2c99b9d13c901",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Complete 3D reconstruction\n",
    "The following function will perform a 3D reconstruction using the building blocks that you have implemented. To reduce the potential error, points that lie unrealistically close or far (`min_distance`/`max_distance`) are considered incorrect matches (outliers) and are discarded. Furthermore, when displaying the 3D point cloud we use a (manually created) image mask to hide background pixels. You can view the full code (and modify it as you wish) in `mvs.py`.\n",
    "\n",
    "You are encouraged to try different support domain sizes, photo consistency functions and input image resolutions.\n",
    "\n",
    "**NOTE** This is a very computationally intensive process which can take a couple of minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "support_domain = 13\n",
    "photo_consistency_function = photo_consistency_SAD\n",
    "image_scale = 0.1 # The original image is very high resolution (4138x6208). Scale it down to improve performance.\n",
    "\n",
    "_, (camera0, camera1), (image0, image1), _ = eth3d_loader.load_eth3d_dataset(\n",
    "    os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"eth3d\", \"statue\"), [\"4\", \"5\"], image_scale, swap_uv=True)\n",
    "statue_mask = helpers.imread_normalized_float_grayscale(os.path.join(helpers.dataset_folder, \"week4\", \"3d reconstruction\", \"eth3d\", \"statue\", \"image4_mask.png\"), image_scale) > 0.5\n",
    "statue_mask = np.swapaxes(statue_mask, 0, 1)\n",
    "\n",
    "# Any points closer/further away from the camera then this distance are considered invalid and will be discarded.\n",
    "min_distance = 1.8\n",
    "max_distance = 4.0\n",
    "\n",
    "estimated_points3D_image = mvs.compute_point_cloud(image0, image1, camera0, camera1, support_domain, photo_consistency_function, closest_point_between_rays, min_distance, max_distance)\n",
    "mvs.plot_point_cloud(image0, estimated_points3D_image, statue_mask, camera0.pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
